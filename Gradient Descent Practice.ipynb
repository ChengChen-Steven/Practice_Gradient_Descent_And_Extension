{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this repository, you are expecting to see the following analyses from scratch:\n",
    "\n",
    "* __Model 1__: use a stat package in python & regularization <br/>\n",
    "* __Model 2__: write gradient descent from scratch  <br/>\n",
    "* __Model 3__: add regularization in model 2 <br/>\n",
    "* __Model 4__: stochastic gradient descent (later)  <br/>\n",
    "* __Model 5__: mini-batch gradient descent (later)  <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Select the dataset: iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width  target\n",
       "0           5.1          3.5           1.4          0.2       1\n",
       "1           4.9          3.0           1.4          0.2       1\n",
       "2           4.7          3.2           1.3          0.2       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the sample dataset from sklearn\n",
    "iris = datasets.load_iris()\n",
    "dat = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "# for simplicity, we only examine binary classification:\n",
    "# map to whether not 1\n",
    "value_map = {0. : 1, \n",
    "             1. : 0, \n",
    "             2. : 1} \n",
    "# replace the value in target & change column names\n",
    "dat['target'] = dat['target'].map(value_map)\n",
    "dat.columns = [var.replace(' (cm)', '') for var in dat.columns if '(cm)' in var] + ['target']\n",
    "dat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.drop(['petal length', 'petal width'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepal length', 'sepal width', 'target'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Use statistical packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.506818\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>target</td>      <th>  No. Observations:  </th>  <td>   150</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   147</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sun, 15 Jul 2018</td> <th>  Pseudo R-squ.:     </th>  <td>0.2038</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>16:56:06</td>     <th>  Log-Likelihood:    </th> <td> -76.023</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -95.477</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>3.557e-09</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sepal length</th> <td>   -0.1404</td> <td>    0.246</td> <td>   -0.570</td> <td> 0.569</td> <td>   -0.623</td> <td>    0.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sepal width</th>  <td>    3.2142</td> <td>    0.642</td> <td>    5.010</td> <td> 0.000</td> <td>    1.957</td> <td>    4.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>    <td>   -8.0251</td> <td>    2.387</td> <td>   -3.362</td> <td> 0.001</td> <td>  -12.703</td> <td>   -3.347</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   No. Observations:                  150\n",
       "Model:                          Logit   Df Residuals:                      147\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Sun, 15 Jul 2018   Pseudo R-squ.:                  0.2038\n",
       "Time:                        16:56:06   Log-Likelihood:                -76.023\n",
       "converged:                       True   LL-Null:                       -95.477\n",
       "                                        LLR p-value:                 3.557e-09\n",
       "================================================================================\n",
       "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "sepal length    -0.1404      0.246     -0.570      0.569      -0.623       0.342\n",
       "sepal width      3.2142      0.642      5.010      0.000       1.957       4.472\n",
       "intercept       -8.0251      2.387     -3.362      0.001     -12.703      -3.347\n",
       "================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Package 1: statsmodels.api\n",
    "import statsmodels.api as sm\n",
    "dat1 = dat.copy()\n",
    "dat1['intercept'] = 1 # Note, if we don't have the intercept, statsmodels will by default has not intercept\n",
    "model = sm.Logit(dat1['target'], dat1.loc[:,dat1.columns!='target'])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept:  [-8.0244352]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sepal length': -0.1403859441267881, 'sepal width': 3.214023049142017}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Package 2: sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "dat2 = dat.copy()\n",
    "lr = LogisticRegression(random_state=0, C=1e6) # lower C means higher penalty, use convention in SVM\n",
    "lr.fit(dat2.loc[:,dat2.columns!='target'], dat2['target'])\n",
    "print('intercept: ', lr.intercept_)\n",
    "dict(zip(dat2.columns[dat2.columns!='target'], lr.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept:  [-6.49918279]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sepal length': -0.230651632407577, 'sepal width': 2.87903841342581}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn with l2 regularization, C=25\n",
    "lr = LogisticRegression(random_state=0, C=25, penalty='l2') # lower C means higher penalty, use convention in SVM\n",
    "lr.fit(dat2.loc[:,dat2.columns!='target'], dat2['target'])\n",
    "print('intercept: ', lr.intercept_)\n",
    "dict(zip(dat2.columns[dat2.columns!='target'], lr.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the result is slightly different, probably because one requires smaller different between iterations to determine the convergence. Therefore, we believe everything is correct here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Build the logistic regression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>0.472984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length  sepal width      target\n",
       "count    150.000000   150.000000  150.000000\n",
       "mean       5.843333     3.054000    0.666667\n",
       "std        0.828066     0.433594    0.472984\n",
       "min        4.300000     2.000000    0.000000\n",
       "25%        5.100000     2.800000    0.000000\n",
       "50%        5.800000     3.000000    1.000000\n",
       "75%        6.400000     3.300000    1.000000\n",
       "max        7.900000     4.400000    1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review the dataset\n",
    "dat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sigmoid functions\n",
    "def sigmoid(x):\n",
    "    return(1/(1+math.exp(-x))) # we should raise exception for different x data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build loss function \n",
    "def lr_loss(y=1, p=0.5):\n",
    "    if (y in [0,1]) and (0<p<1):\n",
    "        return(-y*math.log(p)-(1-y)*math.log(1-p))\n",
    "    else:\n",
    "        raise ValueError('input y or p is out of bound.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train the model\n",
    "def lr_train(data = dat, fit_intercept = True, random_state = 0, alpha = [0.05, 1, 1], tol = 1e-5, target = 'target', varList = []):\n",
    "    # assume there is no column called 'intercept'\n",
    "    if fit_intercept:\n",
    "        dat['intercept'] = 1\n",
    "        varList.append('intercept')\n",
    "    # initiate beta based on random_state:\n",
    "    random.seed(random_state) \n",
    "    init_beta = [0] * 3\n",
    "    new_beta = [random.random() for i in range(len(varList))]\n",
    "    # add two columns: predicted prob \n",
    "    data['pred'] = data.apply(lambda row: sigmoid(np.dot(row[varList], init_beta)), axis = 1)\n",
    "    data['loss'] = data.apply(lambda row: lr_loss(y=row['target'], p=row['pred']), axis=1)\n",
    "    # loop through \n",
    "    # but we will use vectorization\n",
    "    while max(abs(np.array(new_beta) - np.array(init_beta)))>tol:\n",
    "        init_beta = new_beta\n",
    "        loss = np.sum(data[varList].mul(data['target']-data['pred'], axis=0), axis=0)*(1/data.shape[0])\n",
    "        new_beta = np.sum([np.prod([loss, alpha], axis=0), init_beta], axis=0)\n",
    "        data['pred'] = data.apply(lambda row: sigmoid(np.dot(row[varList], new_beta)), axis=1)\n",
    "        print(new_beta)\n",
    "    return(new_beta)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr_train(data=dat, varList = ['sepal length', 'sepal width'])) # , 'petal length', 'petal width'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is something wrong with this chunk of code:\n",
    "    1. it could be parallel computed\n",
    "    2. the running time is really volatile -> could be something wrong with the code here (*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the result <br>\n",
    "[-0.14102365473816966, 3.2122877033154436, -8.01551302010229]\n",
    "<br>\n",
    "It just takes too long to compute. The learning rate should be selected really carefully. \n",
    "\n",
    "Slightly different result, could be the difference in stopping rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: add regularization in model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lr_train_l2reg(data = dat, fit_intercept = True, C = 5e1, random_state = 0, alpha = [0.05, 1, 1], tol = 1e-5, target = 'target', varList = []):\n",
    "    # assume there is no column called 'intercept'\n",
    "    if fit_intercept:\n",
    "        dat['intercept'] = 1\n",
    "        varList.append('intercept')\n",
    "    # initiate beta based on random_state:\n",
    "    random.seed(random_state) \n",
    "    init_beta = [0] * 3\n",
    "    new_beta = [random.random() for i in range(len(varList))]\n",
    "    # new_beta = [0.26,  2.779, -1.2988, 2.703, -7.320] \n",
    "    # add two columns: predicted prob \n",
    "    data['pred'] = data.apply(lambda row: sigmoid(np.dot(row[varList], init_beta)), axis = 1)\n",
    "    data['loss'] = data.apply(lambda row: lr_loss(y=row['target'], p=row['pred']), axis=1)\n",
    "    # loop through\n",
    "    while max(abs(np.array(new_beta) - np.array(init_beta)))>tol:\n",
    "        init_beta = new_beta\n",
    "        loss = np.subtract(np.sum(data[varList].mul(data['target']-data['pred'], axis=0), axis=0), np.dot(2/C,init_beta)) *(1/data.shape[0])\n",
    "        new_beta = np.sum([np.prod([loss, alpha], axis=0), init_beta], axis=0)\n",
    "        data['pred'] = data.apply(lambda row: sigmoid(np.dot(row[varList], new_beta)), axis=1)\n",
    "        print(new_beta)\n",
    "    return(new_beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr_train_l2reg(data=dat, varList = ['sepal length', 'sepal width'])) # , 'petal length', 'petal width'     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the result <br>\n",
    "[-0.23119375274983178, 2.877706303593354, -6.492015452858157]\n",
    "<br>\n",
    "It just takes too long to compute. The learning rate should be selected really carefully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
